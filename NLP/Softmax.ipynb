{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    inpShape = x.shape\n",
    "    if(len(inpShape) ==  1):\n",
    "        # if only dimension, then return e^x/Sum(e^y)\n",
    "        \n",
    "        return(np.exp(x - max(x)) / np.sum(np.exp(x - max(x)))) \n",
    "        # if two dimensionsional, then return e^x/Sum(e^y) for each column\n",
    "    elif (len(inpShape) == 2):\n",
    "            \n",
    "        return np.exp(x - np.max(x,0)) / np.sum(np.exp(x-np.max(x,0)),0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  0],\n",
       "       [ 0,  0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[2,2]])\n",
    "a- np.max(a,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/animo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.18393972,  0.13533528],\n",
       "       [ 0.18393972,  0.13533528]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple test code\n",
    "a = np.array([100,4], dtype = np.float32)\n",
    "b = np.sum(np.exp(a))\n",
    "a/b\n",
    "a = np.array([[1,2],[1,2]])\n",
    "b = np.exp(a)\n",
    "c = np.sum(b,0)\n",
    "a/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71828183,  7.3890561 ],\n",
       "       [ 2.71828183,  7.3890561 ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.43656366,  14.7781122 ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write basic test for the function\n",
    "def test_softmax_basic():\n",
    "    print(\"Basic Tests\")\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print(\"Test 1:\",test1 )\n",
    "    res1 = np.array([0.26894,0.73106])\n",
    "    assert np.allclose(test1,res1, rtol = 1e-05, atol = 1e-06)\n",
    "    \n",
    "    test2 = softmax(np.array([[1,2],[1,2]]))\n",
    "    print(\"Test 2:\",test2 )\n",
    "    res2 = np.array([[0.5,0.5],[ 0.5,  0.5]] )\n",
    "    assert np.allclose(test2,res2, rtol = 1e-05, atol = 1e-06)\n",
    "    \n",
    "    test3 = softmax(np.array([1,2,3,4,5,6,7,8,9]))\n",
    "    print(\"Test 3\")\n",
    "    res3 = np.array([0.00021,0.00058,0.00157,0.00426,0.01158,0.03148,0.08556,0.23257,0.63220])\n",
    "    assert np.allclose(test3,res3, rtol = 1e-5, atol = 1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Tests\n",
      "Test 1: [ 0.26894142  0.73105858]\n",
      "Test 2: [[ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "Test 3\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.73105858,  0.73105858],\n",
       "       [ 0.73105858,  0.73105858]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = softmax(np.array([[1,2],[1,2]]))\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.73105858,  0.88079708],\n",
       "       [ 0.88079708,  0.88079708]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([[1,2],[2,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_sigmoid(x):\n",
    "    #input x has the exponentized function 1/(1+e^-x)\n",
    "    return(x*(1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19661193,  0.10499359,  0.04517666])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_sigmoid(sigmoid(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_sigmoid():\n",
    "    print(\"Running tests\")\n",
    "    input_value = np.array([[1, 2], [-1, -2]])\n",
    "    sigmoid_value = sigmoid(input_value)\n",
    "    gradient = gradient_sigmoid(sigmoid_value)\n",
    "    test_sigmoid_value = np.array([[0.73105858, 0.88079708],[0.26894142, 0.11920292]])\n",
    "    assert np.allclose(sigmoid_value,test_sigmoid_value, rtol=1e-05, atol=1e-06)\n",
    "    print(sigmoid_value)\n",
    "    \n",
    "    test_gradient_value = np.array([[0.19661193, 0.10499359],[0.19661193, 0.10499359]])\n",
    "    assert np.allclose(gradient,test_gradient_value, rtol=1e-05, atol=1e-06)\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_gradient_check(cost_function,input_vector):\n",
    "    #input cost_function is a function that takes an input and returns the cost(sigmoid) and its gradients\n",
    "    #input input_vector is the point at which we check the gradient\n",
    "    \n",
    "    #<TBD>\n",
    "    random_state = random.getstate()\n",
    "    save_state = random.setstate(random_state)\n",
    "    \n",
    "    cost, gradient = cost_function(input_vector)\n",
    "    h = 1e-4\n",
    "    \n",
    "    #The way naive gradient check function would work is to calculate {f(x+h)-f(x)}/h\n",
    "    # That is the definition of a gradient at a point\n",
    "    \n",
    "    #Using np.nditer here. Check the npnditer notebook to work through the syntax\n",
    "    it = np.nditer(input_vector, flags = [\"multi_index\"], op_flags = [\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        i = it.multi_index\n",
    "        \n",
    "        #Calculating naive gradient\n",
    "        input_vector[0] = (cost_function(input_vector[0] + h) - cost_function(input_vector[0])) / h\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15241.383936, 246.91200000000001)\n",
      "(14, array([2, 4, 6]))\n",
      "(0.92800545686052149, array([ -1.67842700e+00,  -9.45992713e-01,   1.55447806e-03]))\n"
     ]
    }
   ],
   "source": [
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "u = quad(np.array(123.456))    \n",
    "print(u)\n",
    "\n",
    "u = quad(np.array([1,2,3]))    \n",
    "print(u)\n",
    "\n",
    "u = quad(np.random.randn(3,))    \n",
    "print(u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
